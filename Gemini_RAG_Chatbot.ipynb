{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIHmp8KCu-OT"
      },
      "source": [
        "### **Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcEoJGP-u33t",
        "outputId": "0daace1e-d777-44d5-ebc6-c65c3df33554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-readers-file 0.2.2 requires pypdf<5.0.0,>=4.0.1, but you have pypdf 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU bitsandbytes\n",
        "!pip install -qU accelerate\n",
        "!pip install -qU cohere llama-index\n",
        "!pip install -qU sentence-transformers\n",
        "!pip install -qU llama-index\n",
        "!pip install -qU langchain\n",
        "!pip install -qU pypdf\n",
        "!pip install -qU transformers\n",
        "!pip install -qU langchain-community\n",
        "!pip install -qU pypdf\n",
        "!pip install -qU tiktoken\n",
        "!pip install -qU llama-index-llms-huggingface\n",
        "!pip install -qU llama-index-embeddings-langchain\n",
        "!pip install -qU pymupdf\n",
        "!pip install -qU faiss-cpu\n",
        "!pip install -qU langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ovGN7zv5Ll"
      },
      "source": [
        "### **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0zUlHhV9v4de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,TextStreamer\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from langchain_community.embeddings import (\n",
        " HuggingFaceEmbeddings\n",
        ")\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from llama_index.core import Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pOO7HTtSx4aN"
      },
      "outputs": [],
      "source": [
        "from google.generativeai import GenerativeModel, configure\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY = \"Your-gemini-api-key-here\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LplpYxJy0aFA"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"<<SYS>>\n",
        "You are an AI assistant specialized in analyzing and explaining academic papers, particularly in the field of machine learning and natural language processing. When answering questions:\n",
        "1. Provide clear, concise, and accurate information based on the given context.\n",
        "2. If the context doesn't contain enough information to fully answer the question, state this clearly and provide the best possible answer with the available information.\n",
        "3. Use academic language and technical terms where appropriate, but also explain complex concepts in simpler terms when necessary.\n",
        "4. If asked about specific sections of the paper (e.g., methodology, results, conclusions), focus your answer on those areas.\n",
        "5. When discussing the paper's contributions or impact, try to place them in the broader context of the field.\n",
        "6. If you're unsure about any information, express your uncertainty rather than making unfounded claims.\n",
        "Your goal is to help users understand the key points, methodologies, and implications of the academic paper in question.\n",
        "<</SYS>>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_inference(question: str, context: str, history: list) -> str:\n",
        "    # Initialize Gemini API (make sure to authenticate and configure your API key)\n",
        "\n",
        "    # Build the prompt based on context and history\n",
        "    if history:\n",
        "        # Format previous questions and answers into a dialogue format\n",
        "        history_prompt = \"\\n\".join(f\"Q: {q}\\nA: {a}\" for q, a in history)\n",
        "        full_prompt = f\"{history_prompt}\\n\\nContext: {context}\\nQ: {question}\"\n",
        "    else:\n",
        "        # If no history, just use context and the current question\n",
        "        full_prompt = f\"Context: {context}\\nQ: {question}\"\n",
        "\n",
        "    # Define the model (use the appropriate Gemini model name)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    # Extract the generated response text from the API result\n",
        "    response = model.generate_content(full_prompt)\n",
        "\n",
        "    response = response.text\n",
        "\n",
        "    history.append((question, response))\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "nWCx6JUMHawI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKnt4Lc81oPX",
        "outputId": "b2efc4a5-577a-4e2a-91dd-e9db94afd8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embed_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_4bhhXqZ1w3B"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "loader = PyMuPDFLoader(\"/content/Searching For Best Practices in RAG.pdf\")\n",
        "raw_documents = loader.load()\n",
        "\n",
        "# Split the documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_documents = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Convert to LangChain Document format\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=doc.page_content\n",
        "    ) for doc in split_documents\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "tLXAboan2SjF"
      },
      "outputs": [],
      "source": [
        "index = FAISS.from_documents(documents,embedding=embed_model,distance_strategy=DistanceStrategy.COSINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5piouKT03AKS"
      },
      "outputs": [],
      "source": [
        "history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAVFZDE53Dt0",
        "outputId": "81c2f9db-8c96-4449-b157-6d5178067754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={}, page_content='queries. The repacking and summarization modules further refine the system’s output, ensuring\\nhigh-quality responses across different tasks.\\n5\\nDiscussion\\n5.1\\nBest Practices for Implementing RAG\\nAccording to our experimental findings, we suggest two distinct recipes or practices for implementing\\nRAG systems, each customized to address specific requirements: one focusing on maximizing\\nperformance, and the other on striking a balance between efficiency and efficacy.\\nBest Performance Practice: To achieve the highest performance, it is recommended to incorporate\\nquery classification module, use the “Hybrid with HyDE” method for retrieval, employ monoT5 for\\nreranking, opt for Reverse for repacking, and leverage Recomp for summarization. This configuration\\nyielded the highest average score of 0.483, albeit with a computationally-intensive process.\\nBalanced Efficiency Practice: In order to achieve a balance between performance and efficiency,'), Document(metadata={}, page_content='query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening\\nprocessing steps: query classification (determining whether retrieval is necessary for a given input\\nquery), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the\\norder of retrieved documents based on their relevance to the query), repacking (organizing the\\nretrieved documents into a structured one for better generation), summarization (extracting key\\ninformation for response generation from the repacked document and eliminating redundancies)\\nmodules. Implementing RAG also requires decisions on the ways to properly split documents into\\nchunks, the types of embeddings to use for semantically representing these chunks, the choice of\\n∗Corresponding Author.\\nPreprint. Under review.\\narXiv:2407.01219v1  [cs.CL]  1 Jul 2024'), Document(metadata={}, page_content='particularly for the entire RAG workflow.\\nIn this study, we aim to identify the best practices for RAG through extensive experimentation. Given\\nthe infeasibility of testing all possible combinations of these methods, we adopt a three-step approach\\nto identify optimal RAG practices. First, we compare representative methods for each RAG step (or\\nmodule) and select up to three of the best-performing methods. Next, we evaluate the impact of each\\nmethod on the overall RAG performance by testing one method at a time for an individual step, while\\nkeeping the other RAG modules unchanged. This allows us to determine the most effective method\\nfor each step based on its contribution and interaction with other modules during response generation.\\nOnce the best method is chosen for a module, it is used in subsequent experiments. Finally, we\\nempirically explore a few promising combinations suitable for different application scenarios where')]\n"
          ]
        }
      ],
      "source": [
        "question = \"Give me all the best practices about RAG mentioned in the paper.Make sure you include all best practices about Query,Indexing,Retrieval and Generation.\"\n",
        "retrieved_docs = index.similarity_search(\n",
        "    question,\n",
        "    k=3\n",
        ")\n",
        "print(retrieved_docs)\n",
        "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgK9naAjKaKv",
        "outputId": "41e7d061-ca9a-4790-a8ad-9614e9962b45"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries. The repacking and summarization modules further refine the system’s output, ensuring\n",
            "high-quality responses across different tasks.\n",
            "5\n",
            "Discussion\n",
            "5.1\n",
            "Best Practices for Implementing RAG\n",
            "According to our experimental findings, we suggest two distinct recipes or practices for implementing\n",
            "RAG systems, each customized to address specific requirements: one focusing on maximizing\n",
            "performance, and the other on striking a balance between efficiency and efficacy.\n",
            "Best Performance Practice: To achieve the highest performance, it is recommended to incorporate\n",
            "query classification module, use the “Hybrid with HyDE” method for retrieval, employ monoT5 for\n",
            "reranking, opt for Reverse for repacking, and leverage Recomp for summarization. This configuration\n",
            "yielded the highest average score of 0.483, albeit with a computationally-intensive process.\n",
            "Balanced Efficiency Practice: In order to achieve a balance between performance and efficiency,\n",
            "query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening\n",
            "processing steps: query classification (determining whether retrieval is necessary for a given input\n",
            "query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the\n",
            "order of retrieved documents based on their relevance to the query), repacking (organizing the\n",
            "retrieved documents into a structured one for better generation), summarization (extracting key\n",
            "information for response generation from the repacked document and eliminating redundancies)\n",
            "modules. Implementing RAG also requires decisions on the ways to properly split documents into\n",
            "chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
            "∗Corresponding Author.\n",
            "Preprint. Under review.\n",
            "arXiv:2407.01219v1  [cs.CL]  1 Jul 2024\n",
            "particularly for the entire RAG workflow.\n",
            "In this study, we aim to identify the best practices for RAG through extensive experimentation. Given\n",
            "the infeasibility of testing all possible combinations of these methods, we adopt a three-step approach\n",
            "to identify optimal RAG practices. First, we compare representative methods for each RAG step (or\n",
            "module) and select up to three of the best-performing methods. Next, we evaluate the impact of each\n",
            "method on the overall RAG performance by testing one method at a time for an individual step, while\n",
            "keeping the other RAG modules unchanged. This allows us to determine the most effective method\n",
            "for each step based on its contribution and interaction with other modules during response generation.\n",
            "Once the best method is chosen for a module, it is used in subsequent experiments. Finally, we\n",
            "empirically explore a few promising combinations suitable for different application scenarios where\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gemini_inference(question, context, history))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "R9WPyI_CI80N",
        "outputId": "08669515-191f-42ad-bb04-8f07eac2368e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper outlines two best practices for implementing RAG (Retrieval-Augmented Generation) systems:\n",
            "\n",
            "**1. Best Performance Practice (Maximizing Performance):**\n",
            "\n",
            "* **Query Classification:**  Use a dedicated query classification module to determine whether retrieval is necessary for a given input query.\n",
            "* **Indexing:**  Employ \"Hybrid with HyDE\" method for retrieval. (The paper does not explicitly explain what \"HyDE\" is, but it implies it is a specific method of indexing.)\n",
            "* **Retrieval:**  Not explicitly mentioned in the text provided. It focuses on reranking, not retrieval itself. \n",
            "* **Reranking:**  Utilize monoT5 for reranking retrieved documents. \n",
            "* **Repacking:**  Opt for \"Reverse\" repacking. \n",
            "* **Summarization:**  Leverage \"Recomp\" for summarization.\n",
            "\n",
            "**2. Balanced Efficiency Practice (Balancing Performance and Efficiency):**\n",
            "\n",
            "* **Query Classification:** Utilize a query classification module to determine retrieval necessity.\n",
            "* **Indexing:**  The paper recommends considering \"query-dependent retrievals\" for efficiency, but does not specify a specific indexing method.  \n",
            "* **Retrieval:**  Emphasize \"query-dependent retrievals\" to improve efficiency.\n",
            "* **Reranking:** The paper does not explicitly mention a specific reranking method for efficiency.\n",
            "* **Repacking:** Not explicitly mentioned in the text provided.\n",
            "* **Summarization:** Not explicitly mentioned in the text provided.\n",
            "\n",
            "**Important Notes:**\n",
            "\n",
            "* **Retrieval:** The paper focuses more on reranking and summarization than retrieval itself. \n",
            "* **Efficiency:**  The \"Balanced Efficiency Practice\" is more about general principles for efficiency than a specific set of methods.\n",
            "* **HyDE:** The paper doesn't explain what \"HyDE\" is, so this specific indexing method needs further research.\n",
            "\n",
            "**Overall:** The paper provides valuable insights into best practices for RAG, but it requires further exploration to fully understand the details of specific methods like \"HyDE\" and the optimal approaches for retrieval within each practice. \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}